{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "spark = SparkSession.builder.appName(\"Python Spark SQL basic example\").config(\"spark.some.config.option\",\"some-value\").getOrCreate()\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Python Spark SQL basic example\").config(\"spark.some.config.option\",\"some-value\").getOrCreate()\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"false\").load('/Users/Joyce/Desktop/Project AMZ/ratings_Electronics.csv')\n",
    "df = df.select(col(\"_c0\").alias(\"userId\"), col(\"_c1\").alias(\"itemId\"), col(\"_c2\").alias(\"rating\"), col(\"_c3\").alias(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.59.188.215:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10bab6390>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_sparsity(itemlimit, userlimit, df):\n",
    "    product = df.groupBy(\"itemId\").count()\n",
    "    product_filter = product.filter(product['count'] > itemlimit)\n",
    "    Data = df.join(product_filter, ['itemId'], 'leftsemi')\n",
    "    \n",
    "    user = Data.groupBy(\"userId\").count()\n",
    "    user_filter = user.filter(user['count'] > userlimit)\n",
    "    DF = Data.join(user_filter, ['userId'], 'leftsemi')\n",
    "    \n",
    "    available = DF.count()\n",
    "    product_total = DF.select(\"itemId\").distinct().count()\n",
    "    user_total = DF.select(\"userId\").distinct().count()\n",
    "    \n",
    "    print(\"available rating: \" + str(available))\n",
    "    print(\"distinct product: \" + str(product_total))\n",
    "    print(\"distinct user: \" + str(user_total))\n",
    "    \n",
    "    result = (float(available)/(float(product_total) * float(user_total)))*100\n",
    "    print(result)\n",
    "    \n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available rating: 3374805\n",
      "distinct product: 208371\n",
      "distinct user: 615996\n",
      "0.002629259887833436\n"
     ]
    }
   ],
   "source": [
    "df=calculate_sparsity(2, 2, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.sql import SQLContext\n",
    "stringIndexer = StringIndexer(inputCol=\"itemId\", outputCol=\"ProductIndex\")\n",
    "user = stringIndexer.fit(df)\n",
    "indexed = user.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexed_distinct=indexed.select(\"userId\").distinct()\n",
    "from pyspark.sql.types import *\n",
    "User_id = sqlContext.createDataFrame(indexed_distinct.rdd.map(lambda x: x[0]).zipWithIndex(), StructType([StructField(\"userId\", StringType(), True),StructField(\"User_ID\", IntegerType(), True)]))\n",
    "join=indexed.join(User_id,indexed.userId == User_id.userId)\n",
    "from pyspark.sql.functions import udf, col, regexp_replace\n",
    "def inte(f):\n",
    "    return int(f)\n",
    "inte_udf = udf(inte)\n",
    "rating_data = join.withColumn('Product_ID', inte_udf(col(\"ProductIndex\")))\n",
    "\n",
    "from pyspark.sql.types import FloatType, IntegerType\n",
    "rating_data = rating_data.withColumn(\"Product_ID\", rating_data[\"Product_ID\"].cast(IntegerType()))\n",
    "rating_data = rating_data.withColumn(\"rating\", rating_data[\"rating\"].cast(FloatType()))\n",
    "rating_cleaned = rating_data.select('User_ID', 'Product_ID', 'rating')\n",
    "rating_final=rating_cleaned.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_rdd, valid_rdd, test_rdd = rating_final.randomSplit(weights=[6, 2, 2], seed=0)\n",
    "train_df = train_rdd.toDF().cache()\n",
    "valid_df = valid_rdd.toDF().cache()\n",
    "test_df = test_rdd.toDF().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2023607"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "items = train_rdd.map(lambda x: x[1]).distinct()\n",
    "#items.take(10)\n",
    "maxIndex = items.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186959"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n",
      "|User_ID|Product_ID|rating|\n",
      "+-------+----------+------+\n",
      "|      0|     13093|   4.0|\n",
      "|      0|     10620|   5.0|\n",
      "|      1|     38247|   5.0|\n",
      "|      2|     14873|   5.0|\n",
      "|      3|     20443|   5.0|\n",
      "|      3|     17759|   5.0|\n",
      "|      3|       264|   5.0|\n",
      "|      5|     26388|   5.0|\n",
      "|      5|        30|   5.0|\n",
      "|      6|     29881|   5.0|\n",
      "|      6|     30026|   5.0|\n",
      "|      6|      4215|   3.0|\n",
      "|      6|    102304|   5.0|\n",
      "|      6|     48013|   5.0|\n",
      "|      6|      6807|   4.0|\n",
      "|      6|    164028|   4.0|\n",
      "|      6|    107023|   4.0|\n",
      "|      6|      8387|   4.0|\n",
      "|      6|      4411|   5.0|\n",
      "|      7|     37380|   3.0|\n",
      "+-------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [(13093, 4.0)]),\n",
       " (0, [(10620, 5.0)]),\n",
       " (1, [(38247, 5.0)]),\n",
       " (2, [(14873, 5.0)]),\n",
       " (3, [(20443, 5.0)]),\n",
       " (3, [(17759, 5.0)])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rdd = train_rdd.map(lambda x: (x.User_ID, [(x.Product_ID, x['rating'])]))\n",
    "train_rdd.take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [(13093, 4.0), (10620, 5.0)]),\n",
       " (512000, [(641, 4.0), (10429, 1.0)]),\n",
       " (8200, [(44888, 4.0)]),\n",
       " (16400, [(2887, 5.0)]),\n",
       " (378200,\n",
       "  [(126412, 5.0), (32209, 4.0), (131053, 3.0), (151806, 3.0), (6769, 3.0)]),\n",
       " (434200, [(289, 5.0)])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rdd = train_rdd.reduceByKey(lambda a, b: a + b)\n",
    "train_rdd.take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, SparseVector(373918, {10620: 5.0, 13093: 4.0})),\n",
       " (512000, SparseVector(373918, {641: 4.0, 10429: 1.0})),\n",
       " (8200, SparseVector(373918, {44888: 4.0})),\n",
       " (16400, SparseVector(373918, {2887: 5.0})),\n",
       " (378200,\n",
       "  SparseVector(373918, {6769: 3.0, 32209: 4.0, 126412: 5.0, 131053: 3.0, 151806: 3.0}))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "train_new = train_rdd.map(lambda x: (x[0], Vectors.sparse(maxIndex*2, x[1])))\n",
    "\n",
    "train_new.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hashed dataset where hashed values are stored in the column 'hashes':\n",
      "+------+--------------------+--------------------+\n",
      "|    id|            features|              hashes|\n",
      "+------+--------------------+--------------------+\n",
      "|     0|(373918,[10620,13...|[[-1.267356126E9]...|\n",
      "|512000|(373918,[641,1042...|[[-3.43236794E8],...|\n",
      "|  8200|(373918,[44888],[...|[[1.155334799E9],...|\n",
      "| 16400|(373918,[2887],[5...|[[-1.343943744E9]...|\n",
      "|378200|(373918,[6769,322...|[[-1.328428554E9]...|\n",
      "|434200|(373918,[289],[5.0])|[[-8.78646106E8],...|\n",
      "|175400|(373918,[50642],[...|[[-1.293627691E9]...|\n",
      "|237600|(373918,[9370,128...|[[-1.877861557E9]...|\n",
      "|449200|(373918,[335,669]...|[[-2.004435784E9]...|\n",
      "|264600|(373918,[1823],[5...|[[-3.2679196E8], ...|\n",
      "| 41000|(373918,[1305,746...|[[5.66116488E8], ...|\n",
      "| 93400|(373918,[2948],[5...|[[-4.09248541E8],...|\n",
      "| 83200|(373918,[1060,120...|[[-1.143997308E9]...|\n",
      "|254000|(373918,[1540,203...|[[-1.96104244E8],...|\n",
      "|417800|(373918,[3430,656...|[[-6.27828543E8],...|\n",
      "|261600|(373918,[24701],[...|[[1.844661706E9],...|\n",
      "|262200|(373918,[6438],[5...|[[8.23874817E8], ...|\n",
      "|475200|(373918,[1232,555...|[[-1.206096413E9]...|\n",
      "|284000|(373918,[2007,107...|[[-5.34983376E8],...|\n",
      "| 73800|(373918,[2604],[4...|[[7.26929723E8], ...|\n",
      "+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Approximately joining dfA and dfB on distance smaller than 0.9:\n",
      "+----+------+------------------+\n",
      "| idA|   idB|   JaccardDistance|\n",
      "+----+------+------------------+\n",
      "| 174| 86960|              0.75|\n",
      "| 174|231779|               0.8|\n",
      "| 174|239717|               0.8|\n",
      "| 174|275803|              0.75|\n",
      "| 261| 29816|               0.8|\n",
      "| 261|555956|0.8333333333333334|\n",
      "| 427|176153|0.8333333333333334|\n",
      "| 427|480918|0.6666666666666667|\n",
      "| 800|602363|               0.5|\n",
      "|1596| 46950|              0.75|\n",
      "|1604|553015|0.6666666666666667|\n",
      "|1975|165811|0.6666666666666667|\n",
      "|2661| 86960|               0.8|\n",
      "|3572| 99203|0.8571428571428572|\n",
      "|3572|480811|               0.8|\n",
      "|3572|541336|0.6666666666666667|\n",
      "|3668|105529|               0.8|\n",
      "|4123| 52868|               0.8|\n",
      "|4123|398717|               0.8|\n",
      "|4643| 11569|               0.8|\n",
      "+----+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "dataA = train_new\n",
    "dfA = spark.createDataFrame(dataA, [\"id\", \"features\"])\n",
    "\n",
    "dataB = train_new\n",
    "dfB = spark.createDataFrame(dataB, [\"id\", \"features\"])\n",
    "\n",
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=5)\n",
    "\n",
    "model = mh.fit(dfA)\n",
    "\n",
    "print(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
    "model.transform(dfA).show()\n",
    "\n",
    "print(\"Approximately joining dfA and dfB on distance smaller than 0.9:\")\n",
    "model.approxSimilarityJoin(dfA, dfA, 0.9, distCol=\"JaccardDistance\")\\\n",
    "    .select(col(\"datasetA.id\").alias(\"idA\"),\n",
    "            col(\"datasetB.id\").alias(\"idB\"),\n",
    "            col(\"JaccardDistance\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "key = Vectors.sparse(512000, [641, 10429], [4.0, 1.0])\n",
    "#key = Vectors.sparse(0, [10620, 13093], [5.0, 4.0])\n",
    "model_df = model.approxNearestNeighbors(dfA, key, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+------------------+\n",
      "|    id|            features|              hashes|           distCol|\n",
      "+------+--------------------+--------------------+------------------+\n",
      "|512000|(373918,[641,1042...|[[-3.43236794E8],...|               0.0|\n",
      "|310373|(373918,[641],[5.0])|[[-3.43236794E8],...|               0.5|\n",
      "|290078|(373918,[641],[5.0])|[[-3.43236794E8],...|               0.5|\n",
      "|201958|(373918,[641],[4.0])|[[-3.43236794E8],...|               0.5|\n",
      "| 33607|(373918,[641],[5.0])|[[-3.43236794E8],...|               0.5|\n",
      "|248038|(373918,[641],[3.0])|[[-3.43236794E8],...|               0.5|\n",
      "|558228|(373918,[641],[5.0])|[[-3.43236794E8],...|               0.5|\n",
      "|100215|(373918,[641],[5.0])|[[-3.43236794E8],...|               0.5|\n",
      "|479884|(373918,[641],[4.0])|[[-3.43236794E8],...|               0.5|\n",
      "|551804|(373918,[641],[5.0])|[[-3.43236794E8],...|               0.5|\n",
      "|396375|(373918,[641],[5.0])|[[-3.43236794E8],...|               0.5|\n",
      "|591221|(373918,[641,4743...|[[-3.43236794E8],...|0.6666666666666667|\n",
      "|436616|(373918,[641,4501...|[[-3.43236794E8],...|0.6666666666666667|\n",
      "|103055|(373918,[641,1612...|[[-1.214341393E9]...|0.6666666666666667|\n",
      "| 70626|(373918,[143,641]...|[[-3.43236794E8],...|0.6666666666666667|\n",
      "|400621|(373918,[641,881]...|[[-1.735217066E9]...|0.6666666666666667|\n",
      "|  8247|(373918,[392,641]...|[[-1.905534369E9]...|0.6666666666666667|\n",
      "| 63831|(373918,[10,641],...|[[-3.43236794E8],...|0.6666666666666667|\n",
      "|437810|(373918,[641,5894...|[[-1.79940787E9],...|0.6666666666666667|\n",
      "|  4840|(373918,[641,1239...|[[-3.43236794E8],...|0.6666666666666667|\n",
      "+------+--------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, SparseVector(5000000, {10620: 5.0, 13093: 4.0})),\n",
       " (512000, SparseVector(5000000, {641: 4.0, 10429: 1.0})),\n",
       " (8200, SparseVector(5000000, {44888: 4.0})),\n",
       " (16400, SparseVector(5000000, {2887: 5.0})),\n",
       " (378200,\n",
       "  SparseVector(5000000, {6769: 3.0, 32209: 4.0, 126412: 5.0, 131053: 3.0, 151806: 3.0})),\n",
       " (434200, SparseVector(5000000, {289: 5.0})),\n",
       " (175400, SparseVector(5000000, {50642: 5.0})),\n",
       " (237600, SparseVector(5000000, {9370: 3.0, 12828: 5.0, 41621: 5.0})),\n",
       " (449200, SparseVector(5000000, {335: 5.0, 669: 5.0})),\n",
       " (264600, SparseVector(5000000, {1823: 5.0}))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "from pyspark.ml.linalg import Vectors\n",
    "train_predict = train_rdd.map(lambda x: (x[0],Vectors.sparse(5000000, x[1])))#(Vectors.sparse(x[0], x[1])))\n",
    "train_predict_id = train_rdd.map(lambda x: ((x[0])))\n",
    "train_predict.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recommend(recommended):\n",
    "    recommended_list = []\n",
    "    self = recommended[0][0].__str__().replace(\"[\",'').replace(']','').replace(\")\",\"\").split(\",\")[1:]\n",
    "    self = self[:int(len(self)/2)]\n",
    "    for i in recommended:\n",
    "        lst = i[0].__str__().replace(\"[\",'').replace(']','').replace(\")\",\"\").split(\",\")[1:]\n",
    "        recommended_list.extend(lst[:int(len(lst)/2)])\n",
    "        recommended_list = list(set(recommended_list))\n",
    "    recommendation = list(set(recommended_list)-set(self))    \n",
    "    return(recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['32236', '188233']\n",
      "512000 []\n",
      "8200 ['87062', '33566', '11953']\n"
     ]
    }
   ],
   "source": [
    "train_to_predict = train_predict.take(5)\n",
    "for i in range(len(train_to_predict)):\n",
    "    model_predicted = model.approxNearestNeighbors(dfA,train_to_predict[i][1], 5)\n",
    "    recommended = model_predicted.select('features').take(model_predicted.count())\n",
    "    print((train_to_predict[i][0]), recommend(recommended))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
